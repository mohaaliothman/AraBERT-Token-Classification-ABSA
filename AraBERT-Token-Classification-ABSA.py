# -*- coding: utf-8 -*-
"""Assiment2-NLP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SqO0pQvmM3fS_MKGLsgSWY_lakbhgS4o

# **Step 1: Install and Import Libraries**
"""

!pip install transformers datasets seqeval evaluate pandas openpyxl --quiet
!pip install arabert --quiet

import pandas as pd
import numpy as np
import torch
from datasets import Dataset, DatasetDict
from transformers import (
    AutoTokenizer,
    AutoModelForTokenClassification,
    DataCollatorForTokenClassification,
    Trainer,
    TrainingArguments
)
import evaluate

"""# **Step 2: Load the Excel File and Preprocess**"""

from google.colab import files
uploaded = files.upload()

import pandas as pd

# Now read the uploaded file
filename = list(uploaded.keys())[0]  # get the filename
df = pd.read_excel(filename, sheet_name="price")  # you can change sheet_name if needed

# Now continue as normal
df["Sentence #"].fillna(method="ffill", inplace=True)

grouped = df.groupby("Sentence #").agg({
    "word": list,
    "aspect": list
}).reset_index()

grouped = grouped.rename(columns={"word": "tokens", "aspect": "ner_tags"})

# Encode labels
label_list = sorted(set(tag for tags in grouped["ner_tags"] for tag in tags))
label2id = {label: i for i, label in enumerate(label_list)}
id2label = {i: label for label, i in label2id.items()}

# Map labels to IDs
grouped["ner_tags"] = grouped["ner_tags"].apply(lambda tags: [label2id[tag] for tag in tags])

from datasets import Dataset, DatasetDict

dataset = Dataset.from_pandas(grouped)
dataset = dataset.train_test_split(test_size=0.2, seed=42)
dataset_dict = DatasetDict({
    "train": dataset["train"],
    "test": dataset["test"]
})

dataset_dict

grouped["ner_tags"]

ner_feature = dataset_dict["train"].features["ner_tags"]
ner_feature

"""# **Step 3: Tokenize and Align Labels**"""

model_checkpoint = "aubmindlab/bert-base-arabertv2"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

# Tokenize and Align Labels
def align_labels_with_tokens(labels, word_ids):
    new_labels = []
    current_word = None
    for word_id in word_ids:
        if word_id != current_word:
            current_word = word_id
            label = -100 if word_id is None else labels[word_id]
            new_labels.append(label)
        elif word_id is None:
            new_labels.append(-100)
        else:
            label = labels[word_id]
            if label % 2 == 1:  # Convert B- to I- for same word pieces
                label += 1
            new_labels.append(label)
    return new_labels

def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"],
        truncation=True,
        is_split_into_words=True,
        padding='max_length',
        max_length=128,
    )
    all_labels = examples["ner_tags"]
    new_labels = []
    for i, labels in enumerate(all_labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        new_labels.append(align_labels_with_tokens(labels, word_ids))
    tokenized_inputs["labels"] = new_labels
    return tokenized_inputs

tokenized_datasets = dataset_dict.map(tokenize_and_align_labels, batched=True)

"""# **Step 4: Define Data Collator and Metric**"""

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
metric = evaluate.load("seqeval")

def compute_metrics(eval_preds):
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]
    true_preds = [[id2label[p] for (p, l) in zip(pred, label) if l != -100]
                  for pred, label in zip(predictions, labels)]
    results = metric.compute(predictions=true_preds, references=true_labels)
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"]
    }

"""# **Step 5: Load Model**"""

model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    num_labels=len(label_list),
    id2label=id2label,
    label2id=label2id
)

"""# **Step 6: Define Trainer**"""

training_args = TrainingArguments(
    output_dir="arabic-modernbert-absa",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

"""# **Step 7: Train and Evaluate**"""

trainer.train()
trainer.evaluate()

"""# **Step 8: Test on New Sentences (Optional)**"""

def predict_sentence(sentence):
    model.eval()
    words = sentence.split()
    inputs = tokenizer(words, return_tensors="pt", is_split_into_words=True, truncation=True)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)[0]

    tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
    prediction_labels = [id2label[p.item()] for p in predictions]

    print("\n Prediction Results:")
    for token, label in zip(tokens, prediction_labels):
        if token not in tokenizer.all_special_tokens:
            print(f"{token:15} -> {label}")

# Example usage
predict_sentence("الأكل كان ممتاز لكن خدمة التوصيل سيئة")
predict_sentence("المكان كان جيد لكن الاستقبال سيء")